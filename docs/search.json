[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bits and Bytes of AI",
    "section": "",
    "text": "Data Wizard with a Spark of Innovation.\n\n\nHello and welcome! My name is Lakshmi, and I am a seasoned data scientist with a passion for utilizing machine learning and AI to solve complex business problems. With over [insert number] years of experience in the field, I have worked on a variety of projects that have enabled me to hone my skills in Python, Pytorch, Keras, scikit-learn, AWS, and many other tools and technologies. Through my work, I have helped organizations achieve significant cost savings, reduce risk, and improve operational efficiency, all while driving innovation and delivering results. In this blog, I hope to share some of my insights and experiences, as well as provide valuable tips and best practices for those looking to navigate the world of data science and machine learning. Thank you for joining me on this journey, and I hope you find the content informative and helpful!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Bits and Bytes of DataScience",
    "section": "",
    "text": "Practical Approach for idenitfying Transactional Anomalies\n\n\n\n\n\n\n\n\n\n\n\n\nLakshmi Putta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPart 1 : Webscraping along with Pipeline\n\n\n\n\n\n\n\n\n\n\n\n\nLakshmi Putta\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHands Extracting Key-Value Pairs from Invoice Images using OCR and Gradio\n\n\n\n\n\n\n\n\n\n\n\n\nLakshmi Putta\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\nLakshmi Putta\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Fraud_Detection_Blog/Transaction_Anomaly_Detector.html",
    "href": "posts/Fraud_Detection_Blog/Transaction_Anomaly_Detector.html",
    "title": "Bits and Bytes of DataScience",
    "section": "",
    "text": "Motivation:\nIn this guide, you’ll learn how to get started with Anomaly Detection using Python, including:\n\nWhat is Anamoly?\nWhat are the challenges of Traditional methods to do Anamoly?\nUsing Isolation Forest for Anamoly Detection.\nHow to explain the results?\n\nProblem and Requirements:\nIn finance Such as Banking , Ecommerce some users perform illegal transactions in the database and those transactions leave records in database. How ever this database is mostly composed of non criminal transactions from the people who comply with the rules of the system they are part of.\nGoal: To identify this Fraudulant Transactions by means of running our Database through Anamoly detection system.\nGood news: Our database has all the records for idenitfy the crime.No need for data gathering.\nBad news: Fraudlent transactions of the data is buried in millions of records and you might not recognise fradualnt transaction in the records.\nChallenges:\nNeed for automation mainly because the sheer size of the records make the manual validation of the data to identify the anomaly is impossible to check all records.\nIf the problem is that the human expersts dont have the time to go through the dataset can we try to automate their decision process?\nRule Based approach - If experts could complie a set of rules for instance transactions that exceed a certain amount are anomaly or transactions of certain type X at certain time Y are considered anolmaly etc. Any type of clearly defined rule to singleout a fradualant transaction. With this rule we can easily implement a system that can easily flag any anamolous transaction.that fullfill the rules of anamoly.\n\nHow ever they are several problems with this approach. Coming up with a good set of rules require a lot of expertise and work rule needs to be validated and for particular dataset at hand.\nIf the dataset format changes or a different dataset is used with different distribution the complete set of rules need to be reworked to reflect the new situation.\n\nFinally and most importantly its impossible to predict to all shapes and forms that anomalies can assume.\nSolution to the above challenge: To address this problem of unknown nature of fraudulent transactions we can make use of data we have in the database. We have to build a system which learns from the data what constitues an anomly.\nWe have to put the requirement that the model has to be unsupervised meaning that when we train this model we will not be able to tell that which are anamoly or not. we dont have any information on that so far.\nMachine Learning approach: first step we train a unsupervised model on our data to learn about\n\nFirst step we train a unsupervised model on our data to learn about distribution of data and to get a sense of what an anamoly is in our context.\nIn step two we use this model to predict the likelihood of a transaction being fraud for every transaction using what it has learned in the above step.\n\nA machine learning model will never give a definitive answer of whether a transaction is anamoly their will be always an inheriant uncertanity in the answers it give us. If our model gives a continous value as anamoly score that refelcts the model certanity of the transaction likely to be anamoly we can choose ourself how wide we want to cast our net while looking for possible anamoly.\nThe output of the model will be a mapping of every transaction to certain a anamoly score. This score reflects how certain our model is of certain transaction as anamoly or not. If we want to consider some subset of transaction we have to set a threshold. the nature of anamoly will help us to consider how wider net we want to perform . Downside is we might not undestand why a transaction is anamoly .thats why we have explanability to the model to understand why model has flagged a certain transaction as anamoly.\nDataset:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import f1_score, precision_score, recall_score, roc_curve\nfrom sklearn.metrics import plot_precision_recall_curve, precision_recall_curve, roc_auc_score\n\nfrom tqdm import tqdm\nimport shap\nshap.initjs()\nplt.rcParams[\"figure.figsize\"] = (12,8)\n\n\n\n\n\n\ndata = pd.read_csv(r'C:\\Users\\MY\\Desktop\\Blogs\\Fraud_Detection_Blog\\PS_20174392719_1491204439457_log.csv')\n\n\ndata.head()\n\n\n\n\n\n  \n    \n      \n      step\n      type\n      amount\n      nameOrig\n      oldbalanceOrg\n      newbalanceOrig\n      nameDest\n      oldbalanceDest\n      newbalanceDest\n      isFraud\n      isFlaggedFraud\n    \n  \n  \n    \n      0\n      1\n      PAYMENT\n      9839.64\n      C1231006815\n      170136.0\n      160296.36\n      M1979787155\n      0.0\n      0.0\n      0\n      0\n    \n    \n      1\n      1\n      PAYMENT\n      1864.28\n      C1666544295\n      21249.0\n      19384.72\n      M2044282225\n      0.0\n      0.0\n      0\n      0\n    \n    \n      2\n      1\n      TRANSFER\n      181.00\n      C1305486145\n      181.0\n      0.00\n      C553264065\n      0.0\n      0.0\n      1\n      0\n    \n    \n      3\n      1\n      CASH_OUT\n      181.00\n      C840083671\n      181.0\n      0.00\n      C38997010\n      21182.0\n      0.0\n      1\n      0\n    \n    \n      4\n      1\n      PAYMENT\n      11668.14\n      C2048537720\n      41554.0\n      29885.86\n      M1230701703\n      0.0\n      0.0\n      0\n      0\n    \n  \n\n\n\n\nAbout Data: step - maps a unit of time in the real world. In this case 1 step is 1 hour of time. Total steps 744 (30 days simulation). type - CASH-IN, CASH-OUT, DEBIT, PAYMENT and TRANSFER.\namount - amount of the transaction in local currency.\nnameOrig - customer who started the transaction\noldbalanceOrg - initial balance before the transaction\nnewbalanceOrig - new balance after the transaction\nnameDest - customer who is the recipient of the transaction\noldbalanceDest - initial balance recipient before the transaction. Note that there is not information for customers that start with M (Merchants).\nnewbalanceDest - new balance recipient after the transaction. Note that there is not information for customers that start with M (Merchants).\nisFraud - This is the transactions made by the fraudulent agents inside the simulation. In this specific dataset the fraudulent behavior of the agents aims to profit by taking control or customers accounts and try to empty the funds by transferring to another account and then cashing out of the system.\nisFlaggedFraud - The business model aims to control massive transfers from one account to another and flags illegal attempts. An illegal attempt in this dataset is an attempt to transfer more than 200.000 in a single transaction.\n\ndata['isFraud'].value_counts()/len(data)\n\n0    0.998709\n1    0.001291\nName: isFraud, dtype: float64\n\n\nFeature Engineering:\n\nfeatures = pd.DataFrame(index = data.index)\n\n\nnumerical_columns = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']\n\n\nfeatures[numerical_columns] = data[numerical_columns]\n\n\nfeatures['changebalanceOrig'] = features['newbalanceOrig'] - features['oldbalanceOrg']\nfeatures['changebalanceDest'] = features['newbalanceDest'] - features['oldbalanceDest']\n\n\nfeatures['hour'] = data['step'] % 24\n\n\ntype_one_hot = pd.get_dummies(data['type'])\nfeatures = pd.concat([features, type_one_hot], axis=1)\n\nIsolation Forest:\nModel:\n\nfrom sklearn.ensemble import IsolationForest\n\nforest = IsolationForest(random_state = 0)\nforest.fit(features)\n\nIsolationForest(random_state=0)\n\n\n\nscores = forest.score_samples(features)\n\n\nplt.hist(scores,bins =50)\nplt.ylabel('Number of Transactions',fontsize = 15)\nplt.xlabel('Anamoly Score',fontsize = 15)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)\n\n(array([      0.,  200000.,  400000.,  600000.,  800000., 1000000.]),\n [Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, ''),\n  Text(0, 0, '')])\n\n\n\n\n\nTop Outliers:\n\ntop_n_outliers = 5\ntop_n_outlier_indices = np.argpartition(scores, top_n_outliers)[:top_n_outliers].tolist()\ntop_outlier_entries = data.iloc[top_n_outlier_indices, :]\ntop_outlier_features = features.iloc[top_n_outlier_indices, :]\ntop_outlier_features\n\n\n\n\n\n  \n    \n      \n      amount\n      oldbalanceOrg\n      newbalanceOrig\n      oldbalanceDest\n      newbalanceDest\n      changebalanceOrig\n      changebalanceDest\n      hour\n      CASH_IN\n      CASH_OUT\n      DEBIT\n      PAYMENT\n      TRANSFER\n    \n  \n  \n    \n      4157895\n      37387628.10\n      862621.88\n      0.0\n      37843025.59\n      7.523065e+07\n      -862621.88\n      37387628.10\n      15\n      0\n      0\n      0\n      0\n      1\n    \n    \n      4146715\n      38874009.46\n      435622.96\n      0.0\n      73634878.73\n      1.125089e+08\n      -435622.96\n      38874009.45\n      15\n      0\n      0\n      0\n      0\n      1\n    \n    \n      4153782\n      32572996.87\n      279467.23\n      0.0\n      84947676.90\n      1.175207e+08\n      -279467.23\n      32572996.87\n      15\n      0\n      0\n      0\n      0\n      1\n    \n    \n      4142016\n      51141938.17\n      310058.79\n      0.0\n      87081824.45\n      1.382238e+08\n      -310058.79\n      51141938.17\n      15\n      0\n      0\n      0\n      0\n      1\n    \n    \n      4145988\n      31866120.39\n      503019.94\n      0.0\n      63539675.65\n      9.540580e+07\n      -503019.94\n      31866120.39\n      15\n      0\n      0\n      0\n      0\n      1\n    \n  \n\n\n\n\n\n## Isolation forest anamoly score\nfpr_forest,tpr_forest,threshold_forest = roc_curve(data['isFraud'],-scores)\nauc_score_forest = roc_auc_score(data['isFraud'],-scores)\n\n## Naive Bayes Anamoly Score\nfpr_naive,tpr_naive,threshold_naive =  roc_curve(data['isFraud'], data['amount'])\nauc_score_naive = roc_auc_score(data['isFraud'],data['amount'])\n\n\ndef plot_roc_curve(fpr,tpr,name,auc_score):\n  plt.plot(fpr,tpr,label = name+'AUC={}'.format(round(auc_score,3)))\n\nResults:\n\nplot_roc_curve(fpr_forest,tpr_forest,'islationForest',auc_score_forest)\nplot_roc_curve(fpr_naive,tpr_naive,'Naive Bayes',auc_score_naive)\nplot_roc_curve([0, 1], [0, 1], 'Random guessing', 0.5)\nplt.xlabel('False positive rate', fontsize=15)\nplt.ylabel('True positive rate (recall)', fontsize=15)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.legend(prop={'size': 12})\nplt.legend()\n\n<matplotlib.legend.Legend at 0x22633a36ac0>\n\n\n\n\n\nExplanability:\n\n import shap\nexplainer = shap.TreeExplainer(forest)\n\n\nrandom_indices = np.random.choice(len(features),5000)\nshap_values_random = explainer.shap_values(features.iloc[random_indices,:])\nrandom_features = features.iloc[random_indices,:]\n\nExplanation for local points\n\nshap.initjs()\n\n\n\n\n\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values_random[1, :], random_features.iloc[1, :])\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -> Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\nThe above plot shows how the individual features of the data point contributed to shifting the model output from its expected to base value. These contributing values are called SHAP values. Blue values are contributing to lower the anamoly score and the red values for higher anamoly score.\nFor the above entry features such as amount(higher amount) is suggestive that the data is anamolous.\nSHAP values are local explanations of different features for a given data point. To gain the global trends by aggregating the local explanations.\nSHAP library has a library called dependence plot which shows us how mulitple datapoints of shap value depend on the feature .\n\nshap.dependence_plot('changebalanceDest',shap_values_random,random_features,interaction_index= None,\n                    xmax= 'percentile(99)')\n\n\n\n\n\nshap.dependence_plot(\n 'changebalanceDest',\n shap_values_random,\n random_features,\n interaction_index='CASH_OUT',\n xmax='percentile(99)'\n)\n\n\n\n\n\nshap.dependence_plot(\n 'hour',\n shap_values_random,\n random_features,\n interaction_index='PAYMENT',\n xmax='percentile(99)'\n)\n\n\n\n\nConclusion:\nThe above guide demonstrates how to get started with Anomaly Detection using Python and the Isolation Forest algorithm. The guide explains the challenges of traditional methods for anomaly detection, such as rule-based approaches, and provides a solution to these challenges by using a machine learning approach. The guide uses the Isolation Forest algorithm to train a model on the data and predict the likelihood of a transaction being fraudulent based on the learned information. The guide also explains how to evaluate the results and provides code for visualizing the results using SHAP. The conclusion of the guide is that using a machine learning approach, specifically the Isolation Forest algorithm, can effectively identify fraudulent transactions in large datasets and provide a certain level of certainty in its predictions."
  },
  {
    "objectID": "posts/Handson_Web_Scraping/Handson_Web_Scraping_Notebooks.html",
    "href": "posts/Handson_Web_Scraping/Handson_Web_Scraping_Notebooks.html",
    "title": "Bits and Bytes of DataScience",
    "section": "",
    "text": "Introduction:\nWeb scraping is an automated process to extract information from the internet. It is a powerful tool that can help businesses and individuals gather useful information from various sources on the internet. The data can be used for various purposes such as market research, sentiment analysis, trend analysis, etc. In this blog, we will discuss why web scraping is necessary and how it can be done using Python.\nMotivation:\nWith the proliferation of the internet and the increasing availability of information online, there is a need for an efficient way to collect and analyze this information. Web scraping offers a solution to this problem by allowing users to extract large amounts of data from the internet quickly and easily. This data can then be used for various purposes such as market research, trend analysis, and competitor analysis, to name a few.\nWhy we need webscraping?\nWeb scraping is essential because it allows us to collect and analyze data that is not readily available in a structured format. It helps us in understanding market trends, product reviews, and pricing strategies. It also enables us to gather data that can help us gain a competitive advantage in the market. Web scraping can be used for a wide range of applications such as e-commerce, data analysis, and sentiment analysis.\nFor example, imagine you own an online clothing store and want to monitor the prices of a popular competitor’s clothing items. You could use a web scraper to extract the prices of those items from their website and store the data in a spreadsheet. Then, you could set up an automated notification to alert you when a price drops below a certain threshold. This would give you a competitive advantage by allowing you to adjust your prices accordingly.\nIn this way, web scraping can save you time, help you make more informed decisions, and give you a leg up on the competition.\nExample of the webscraping using below code:\nThe following code is an example of how web scraping can be done using Python. The code scrapes a webpage and extracts all the tables from the webpage using BeautifulSoup. It then extracts the country sex ratios from the tables and cleans the data to retrieve ISO country codes from a GitHub repository.\n\n\nimport math\nimport re\nimport urllib3\nfrom typing import List, Tuple\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n\ndef get_webpage_tables(url: str) -> List[BeautifulSoup]:\n    \"\"\"\n    Downloads a webpage from the given url and extracts all the tables using BeautifulSoup.\n\n    Parameters:\n        url (str): The url of the webpage to download.\n\n    Returns:\n        List[BeautifulSoup]: A list of BeautifulSoup objects, one for each table on the webpage.\n    \"\"\"\n    http = urllib3.PoolManager()\n    response = http.request(\"GET\", url)\n    soup = BeautifulSoup(response.data, features=\"html.parser\")\n    tables = soup.find_all('table')\n    return tables\n\n\ndef process_num(num: str) :\n    \"\"\"\n    Processes a string representing a number to a float.\n\n    Parameters:\n        num (str): A string representing a number.\n\n    Returns:\n        Union[float, str]: The float representation of the input string, or the original string if it cannot be converted.\n    \"\"\"\n    try:\n        return float(re.sub(r'[^\\d.-]', '', num))\n    except ValueError:\n        return num\n\n\n\ndef is_valid_cell(cells: List[BeautifulSoup]) -> bool:\n    \"\"\"\n    Determines if a list of BeautifulSoup objects representing table cells is valid.\n\n    A cell is considered invalid if its text content is 'N/A'.\n\n    Parameters:\n        cells (List[BeautifulSoup]): A list of BeautifulSoup objects representing table cells.\n\n    Returns:\n        bool: True if all the cells are valid, False otherwise.\n    \"\"\"\n    return all(cell.text.strip() != 'N/A' for cell in cells)\n\n\n\ndef make_str_valid(s: str) -> str:\n    \"\"\"\n    Makes a string valid by removing any text in parentheses.\n\n    Parameters:\n        s (str): The string to make valid.\n\n    Returns:\n        str: The input string with any text in parentheses removed.\n    \"\"\"\n    ind_delim = s.find('(')\n    if ind_delim != -1:\n        wrd = s[:ind_delim-1]\n    else:\n        wrd = s\n    return wrd\n\n\ndef extract_country_sex_ratio(tables: List[BeautifulSoup]) -> pd.DataFrame:\n    \"\"\"\n    Extracts country sex ratios from a list of BeautifulSoup tables.\n\n    Parameters:\n        tables (List[BeautifulSoup]): A list of BeautifulSoup tables.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame containing the extracted country sex ratio data.\n    \"\"\"\n    data = []\n    for table in tables:\n        rows = table.find_all('tr')\n        for row in rows:\n            cells = row.find_all('td')\n            if len(cells) > 1 and is_valid_cell(cells):\n                country = make_str_valid(cells[0].text.strip())\n                sex_ratio = process_num(cells[-1].text.strip())\n                data.append([country, sex_ratio])\n\n    df = pd.DataFrame(data, columns=['Country', 'Sex-Ratio'])\n    return df\n\n\n\ndef get_country_codes(df) ->  pd.DataFrame:\n    \"\"\"\n    Retrieves ISO country codes from a GitHub repository and returns a cleaned DataFrame\n    \"\"\"\n    country_raw = pd.read_csv('https://raw.githubusercontent.com/lukes/ISO-3166-Countries-with-Regional-Codes/master/all/all.csv')\n    df_c = country_raw.iloc[:, [0,2]]\n    df_c = df_c.rename(columns={'name':'Country', 'alpha-3':'ISO-code'})\n    df_c = df_c.drop_duplicates(subset=['Country'], keep='last').dropna()\n\n    # Manually modify some of the country names to match the names in the main DataFrame\n    df_c.at[df_c[df_c['Country']=='Viet Nam'].index.values[0], 'Country'] = 'Vietnam'\n    df_c.at[df_c[df_c['Country']=='United States of America'].index.values[0], 'Country'] = 'United States'\n    df_c.at[df_c[df_c['Country']=='Iran (Islamic Republic of)'].index.values[0], 'Country'] = 'Iran'\n    df_c.at[df_c[df_c['Country']=='Russian Federation'].index.values[0], 'Country'] = 'Russia'\n    df_c.at[df_c[df_c['Country']=='United Kingdom of Great Britain and Northern Ireland'].index.values[0], 'Country'] = 'United Kingdom'\n    df_c.at[df_c[df_c['Country']=='Venezuela (Bolivarian Republic of)'].index.values[0], 'Country'] = 'Venezuela'\n    df_c.at[df_c[df_c['Country']==\"Korea (Democratic People's Republic of)\"].index.values[0], 'Country'] = 'Korea, North'\n    df_c.at[ df_c[df_c['Country']=='Korea, Republic of'].index.values[0], 'Country' ] = 'Korea, South'\n    df_c.at[ df_c[df_c['Country']=='Bolivia (Plurinational State of)'].index.values[0], 'Country' ] = 'Bolivia'\n    df_c.at[ df_c[df_c['Country']=='Côte d\\'Ivoire'].index.values[0], 'Country' ] = 'Ivory Coast'\n    df_c.at[ df_c[df_c['Country']=='Congo'].index.values[0], 'Country' ] = 'Congo, Republic of the'\n    df_c.at[ df_c[df_c['Country']=='Tanzania, United Republic of'].index.values[0], 'Country' ] = 'Tanzania'\n    # Using the ISO-3166 coding standard to map countries\n    df['ISO-code'] = df['Country'].map(df_c.set_index('Country')['ISO-code'])\n    # Clean data-frame ( Duplicates & NaNs )\n    df.isna().sum() \n    df = df.dropna()\n    return df\n\n\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\n\n\nurl = 'https://en.wikipedia.org/wiki/List_of_countries_by_sex_ratio'\n\n# define pipeline\npipeline = Pipeline([\n    ('get_tables', FunctionTransformer(get_webpage_tables)),\n    ('extract_ratio', FunctionTransformer(extract_country_sex_ratio)),\n    ('get_codes', FunctionTransformer(get_country_codes))\n])\n\n# apply pipeline to input URL\ndf = pipeline.transform(url)\n\ndf\n\n/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py:842: InsecureRequestWarning:\n\nUnverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Country\n      Sex-Ratio\n      ISO-code\n    \n  \n  \n    \n      1\n      Afghanistan\n      1.03\n      AFG\n    \n    \n      2\n      Albania\n      0.98\n      ALB\n    \n    \n      3\n      Algeria\n      1.03\n      DZA\n    \n    \n      4\n      American Samoa\n      1.0\n      ASM\n    \n    \n      5\n      Andorra\n      1.06\n      AND\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      225\n      Wallis and Futuna\n      1.04\n      WLF\n    \n    \n      227\n      Western Sahara\n      0.99\n      ESH\n    \n    \n      228\n      Yemen\n      1.02\n      YEM\n    \n    \n      229\n      Zambia\n      1.0\n      ZMB\n    \n    \n      230\n      Zimbabwe\n      0.96\n      ZWE\n    \n  \n\n209 rows × 3 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nimport plotly.express as px\n\nthres = 1.3\ndf_th = df.drop(df[ df['Sex-Ratio'] > thres ].index)\n\n# color pallete @ https://plotly.com/python/builtin-colorscales/\nfig = px.choropleth(df_th, locations='ISO-code',\n                color=\"Sex-Ratio\", hover_name=\"Country\",\n                    color_continuous_scale=px.colors.sequential.Sunset, projection=\"natural earth\")\nfig.update_layout(title={'text':'Sex-Ratio per country', 'y':0.95, 'x':0.5, 'xanchor':'center', 'yanchor':'top'})\nfig.show()\n\n\n\n\n\n                                \n                                            \n\n\n\n\n\nimport plotly.express as px\n\nthres = 1.3\ndf_th = df.drop(df[ df['Sex-Ratio'] > thres ].index)\n\n# color pallete @ https://plotly.com/python/builtin-colorscales/\nfig = px.choropleth(df_th, locations='ISO-code',\n                color=\"Sex-Ratio\", hover_name=\"Country\",\n                    color_continuous_scale=px.colors.sequential.Sunset, projection=\"orthographic\")\nfig.update_layout(title={'text':'Sex-Ratio per country', 'y':0.95, 'x':0.5, 'xanchor':'center', 'yanchor':'top'})\nfig.show()"
  },
  {
    "objectID": "posts/Invoice_Recognizer/Invoice_Recognizer.html",
    "href": "posts/Invoice_Recognizer/Invoice_Recognizer.html",
    "title": "Bits and Bytes of DataScience",
    "section": "",
    "text": "Introduction:\nHave you ever been in a situation where you have a scanned copy of an invoice, but you need to extract certain key information from it? It can be a tedious task to manually go through the invoice and extract the information you need. That’s where the Invoice Recognizer comes in handy. In this blog post, we will be discussing the implementation of the Invoice Recognizer using Python.\nObjective:\nThe objective of this project is to extract key-value pairs from an invoice image using the Tesseract OCR engine and regular expressions. We will be using Gradio to build a user interface for this project.\nImplementation:\nTo implement the Invoice Recognizer, we will be using the following libraries:\n\nGradio: For building the user interface.\npytesseract: For OCR (Optical Character Recognition).\nPIL (Python Imaging Library): For working with images.\npandas: For displaying the output in a tabular format.\n\nBackground Information:\nOptical Character Recognition (OCR): Optical Character Recognition, or OCR, is the process of converting scanned images of text into machine-encoded text that can be searched, indexed, and manipulated by a computer. OCR software is used to read the text from images and convert it into machine-readable text. OCR technology is used in many applications, such as digitizing books and documents, recognizing license plates, and scanning receipts.\nTesseract OCR: Tesseract OCR is a widely used open-source OCR engine developed by Google. It is known for its accuracy and ability to recognize text in a wide range of languages. Tesseract OCR is available in many programming languages, including Python.\nGradio: Gradio is a Python library that allows developers to quickly and easily create customizable UI components for their machine learning models. With Gradio, developers can create web interfaces for their models without needing to write any front-end code. Gradio is built on top of Flask and React, making it easy to integrate into existing Python projects. Let’s start by installing the required libraries:\n\n!pip install gradio\n!apt-get update\n!apt-get install tesseract-ocr\n!apt-get install libtesseract-dev\n!pip install pytesseract\n\n\n\n\nimport pytesseract\nfrom PIL import Image\n\npytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n\nOnce we have installed the required libraries, let’s import them into our Python script:\n\nimport gradio as gr\nimport re\nimport pytesseract\nfrom PIL import Image\nimport pandas as pd\n\n\n\n\nWe have imported the necessary libraries, and we have also specified the path to the Tesseract OCR engine. The next step is to define the regular expressions that we will use to match the keys in the invoice.\n\ndef extract_kvp(text,key_regexes):\n    # Define the regular expressions to match the keys\n    key_regexes = key_regexes\n    \"\"\"# [\n        re.compile(r\"^BILLTO\"),\n        re.compile(r\"^Invoice #\"),\n        re.compile(r\"^Task\"),\n        re.compile(r\"^Item\"),\n        re.compile(r\"^Rate\"),\n      #  re.compile(r\"^Price\"),\n        re.compile(r\"^Amount Due \\(USD\\)\"),\n    ]\"\"\"\n\n    # Split the text into lines\n    lines = text.split(\"\\n\")\n\n    # Iterate over the lines and extract the key-value pairs\n    kvp = {}\n    key = None\n    for line in lines:\n        for regex in key_regexes:\n            if regex.match(line):\n                key = regex.pattern.replace(\"^\", \"\").strip()\n                kvp[key] = \"\"\n                break\n        if key:\n            kvp[key] += line + \"\\n\"\n    if key == 'Amount Due \\\\(USD\\\\)':\n      key = 'Amount Due (USD):'\n      kvp[key] = line.split(\":\")[-1].strip().split(\"$\")[-1].strip()\n      \n    else:\n        kvp[key] = line.strip().replace(\",\",\"\").replace(key + \":\", \"\")\n    return kvp\n\nThe function first splits the text into lines using the split method. It then iterates over the lines and matches each line against the regular expressions in key_regexes. When a match is found, it sets key to the corresponding key and initializes an empty string for the value in the kvp dictionary. It then continues to append the subsequent lines to the value until it finds the next key.\nThe function also includes a special case for the “Amount Due (USD)” key, since the value for this key is not on a separate line. It extracts the value using string manipulation and adds it to the kvp dictionary.\nFinally, the function removes any commas and replaces the key and colon from the value using the replace method.\nformat_table The format_table function takes in a dictionary of key-value pairs and formats it as an HTML table using the Pandas library.\n\ndef format_table(output):\n    kvp = {key.strip(): value.replace(\"\\n\", \", \") for key, value in output.items()}\n    print(kvp)\n    df = pd.DataFrame(list(kvp.items()), columns=[\"Key\", \"Value\"])\n    return df.to_html(index=False, justify='left')\n\nThe function first creates a new dictionary (kvp) with stripped keys and values that have newlines replaced with commas. It then creates a Pandas dataframe with the dictionary and formats it as an HTML table using the to_html method. It returns the formatted table as a string.\nextract_kvp_from_image: The extract_kvp_from_image function is the main function that takes in an image and a string of regular expressions, extracts the text from the image, and extracts the key-value pairs using the extract_kvp function. It then formats the output as an HTML table using the format_table function.\n\n\ndef extract_kvp_from_image(image, key_regexes_str):\n    key_regexes = [re.compile(regex) for regex in key_regexes_str.split(\", \")]\n    text = extract_text_from_image(image)\n    print('Extracted Text Data',text)\n    kvp = extract_kvp(text, key_regexes)\n    output = format_table(kvp)\n    return output\n\nNext, we define a function to extract text from the invoice image using the Tesseract OCR engine.\n\ndef extract_text_from_image(image):\n    # Extract the text from the image using Tesseract\n    text = pytesseract.image_to_string(image)\n    \n    return text\n\n\n\n\nAnd finally, we have the extract_kvp_from_image function. This function takes in an image and a string of regular expressions for the keys that we want to extract from the image. It then uses the extract_text_from_image function to extract the text from the image, and the extract_kvp function to extract the key-value pairs from the text using the provided regular expressions. Finally, the function formats the output as an HTML table using the format_table function and returns it.\nLet’s put all the functions together and run the application!\n\nimport pytesseract\nfrom PIL import Image\nimport pandas as pd\nimport re\nimport gradio as gr\n\npytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'\n\ndef extract_text_from_image(image):\n    # Extract the text from the image using Tesseract\n    text = pytesseract.image_to_string(image)\n    \n    return text\n\ndef extract_kvp(text,key_regexes):\n    # Define the regular expressions to match the keys\n    key_regexes = key_regexes\n\n    # Split the text into lines\n    lines = text.split(\"\\n\")\n\n    # Iterate over the lines and extract the key-value pairs\n    kvp = {}\n    key = None\n    for line in lines:\n        for regex in key_regexes:\n            if regex.match(line):\n                key = regex.pattern.replace(\"^\", \"\").strip()\n                kvp[key] = \"\"\n                break\n        if key:\n            kvp[key] += line + \"\\n\"\n    if key == 'Amount Due \\\\(USD\\\\)':\n      key = 'Amount Due (USD):'\n      kvp[key] = line.split(\":\")[-1].strip().split(\"$\")[-1].strip()\n      \n    else:\n        kvp[key] = line.strip().replace(\",\",\"\").replace(key + \":\", \"\")\n    return kvp\n\ndef format_table(output):\n    kvp = {key.strip(): value.replace(\"\\n\", \", \") for key, value in output.items()}\n    df = pd.DataFrame(list(kvp.items()), columns=[\"Key\", \"Value\"])\n    return df.to_html(index=False, justify='left')\n\ndef extract_kvp_from_image(image, key_regexes_str):\n    key_regexes = [re.compile(regex) for regex in key_regexes_str.split(\", \")]\n    text = extract_text_from_image(image)\n    kvp = extract_kvp(text, key_regexes)\n    output = format_table(kvp)\n    return output\n\ninputs = [gr.inputs.Image(type=\"filepath\", label=\"Input\"), gr.inputs.Textbox(label=\"Key Regexes\", default=\"^BILLTO, ^Invoice #, ^Task, ^Item, ^Rate, ^Price, ^Amount Due \\\\(USD\\\\)\")]\n\noutputs = gr.outputs.HTML(label=\"Output\")\n\ngr.Interface(\nextract_kvp_from_image, inputs, title=\"Extract Key-Value Pairs from Invoice Image\",layout =\"browse\", outputs=outputs\n).launch()\n\nConclusion:\nIn this blog post, we have explored how to extract key-value pairs from an invoice image using Python and Gradio. We have used the Tesseract OCR engine to extract text from the image, and regular expressions to extract the key-value pairs from the text. We have also used Gradio to create a user-friendly interface for our application."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]